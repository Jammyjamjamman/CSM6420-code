{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Data Using a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import random\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "\n",
    "# Set random seed for reproducibility.\n",
    "# seed = random.randint(0, 2**32)\n",
    "seed = 2690082906\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Note that data is already 1-hot encoded.\n",
    "train = pd.read_csv(\"train.csv\", index_col=None).astype(\"float32\")\n",
    "\n",
    "train = shuffle(train)\n",
    "train.head(10)\n",
    "\n",
    "# Split data into train and test data.\n",
    "X, Y = train.iloc[:,1:], to_categorical(train[\"Class\"])\n",
    "input_len, input_sz = X.shape[0], X.shape[1]\n",
    "# seed = 3983249514\n",
    "# np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import GaussianDropout, AlphaDropout, GaussianNoise\n",
    "from keras.optimizers import RMSprop, SGD, Adamax\n",
    "from keras.initializers import Constant\n",
    "\n",
    "# CNN model.\n",
    "def create_model(activation = 'relu',\n",
    "                 momentum = 0.1,\n",
    "                 decay_rate = 0.1,\n",
    "                 dropout_rate = 0.2,\n",
    "                 learn_rate = 0.1,\n",
    "                 weight_constraint = None,\n",
    "                 neurons = 5,\n",
    "                 init = 'normal',\n",
    "                 seed = None,\n",
    "                 optimiser = 'adamax'):\n",
    "    \n",
    "    if seed != None:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    # create model\n",
    "    global input_sz\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_dim = input_sz, kernel_initializer=init, activation=activation))\n",
    "    model.add(GaussianDropout(dropout_rate))\n",
    "    model.add(Dense(2, input_dim = neurons, kernel_initializer=Constant(0.5), activation='softmax'))\n",
    "    \n",
    "    optimiser = SGD(lr=learn_rate,  momentum=momentum, decay = decay_rate)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimiser, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def make_prediction(model):\n",
    "    test = pd.read_csv(\"test.csv\")\n",
    "    X_test, Id = test.iloc[:,1:], test[\"TestId\"]\n",
    "\n",
    "    Y_pred = model.predict(X_test)[:,1]#.flatten()\n",
    "    #Y_predt = clf.predict_proba(X_train)[:,1]\n",
    "    print(Y_pred.shape)\n",
    "    pred_df = pd.DataFrame(Id)\n",
    "    pred_df = pred_df.join(pd.DataFrame({\"PredictedScore\": Y_pred}))\n",
    "    pred_df.to_csv(\"predcnn.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.optimizers import SGD\n",
    "from keras.constraints import maxnorm\n",
    "\n",
    "# Grid Searching with Keras.\n",
    "# Code based off the following source:\n",
    "########################################\n",
    "# Title: How to Grid Search Hyperparameters for Deep Learning Models in Python With Keras\n",
    "# Author: Jason Brownlee\n",
    "# Date: 09/08/2018\n",
    "# Available: https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/\n",
    "########################################\n",
    "#\n",
    "# Number of grid searches\n",
    "kfolds = 8\n",
    "\n",
    "def grid_search_learning_params():\n",
    "    global X, Y\n",
    "\n",
    "    activation = ['relu']\n",
    "    momentum = dropout_rate = np.linspace(0, 0.5, 3)\n",
    "    learn_rate = [0.001, 0.01, 0.1, 0.5]\n",
    "    decay_rate = [0.1, 0.005, 0.00005, 0]\n",
    "    neurons = [20]\n",
    "    init = ['normal']\n",
    "\n",
    "    epochs = [200]\n",
    "    batch_size = [2000]\n",
    "    param_grid = dict(activation = activation,\n",
    "                      momentum = momentum,\n",
    "                      learn_rate = learn_rate,\n",
    "                      decay_rate = decay_rate,\n",
    "                      dropout_rate = dropout_rate,\n",
    "                      neurons = neurons,\n",
    "                      epochs = epochs,\n",
    "                      batch_size = batch_size)\n",
    "    \n",
    "    model = KerasClassifier(build_fn=create_model)\n",
    "    grid = GridSearchCV(estimator=model, cv=kfolds, param_grid=param_grid, n_jobs=1)\n",
    "    grid_result = grid.fit(X, Y) \n",
    "    # summarize results\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_neurons():\n",
    "    global X, Y\n",
    "    \n",
    "    # Modify these variables, depending on the previous best parameters found in the grid search.\n",
    "    activation = ['relu']\n",
    "    momentum = [0.25]\n",
    "    dropout_rate = [0]\n",
    "    learn_rate = [0.5]\n",
    "    decay_rate = [5e-05]\n",
    "    neurons = [1,5,10,20,25,30]\n",
    "    init = ['normal']\n",
    "\n",
    "    epochs = [200]\n",
    "    batch_size = [2000]\n",
    "    param_grid = dict(activation = activation,\n",
    "                      momentum = momentum,\n",
    "                      learn_rate = learn_rate,\n",
    "                      decay_rate = decay_rate,\n",
    "                      dropout_rate = dropout_rate,\n",
    "                      neurons = neurons,\n",
    "                      epochs = epochs,\n",
    "                      batch_size = batch_size)\n",
    "    \n",
    "    model = KerasClassifier(build_fn=create_model)\n",
    "    grid = GridSearchCV(estimator=model, cv=kfolds, param_grid=param_grid, n_jobs=1)\n",
    "    grid_result = grid.fit(X, Y) \n",
    "    # summarize results\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_activation():\n",
    "    global X, Y\n",
    "    activation = ['hard_sigmoid', 'sigmoid', 'tanh', 'relu', 'linear']\n",
    "    momentum = [0.25]\n",
    "    dropout_rate = [0]\n",
    "    learn_rate = [0.5]\n",
    "    decay_rate = [5e-05]\n",
    "    neurons = [25]\n",
    "    init = ['normal']\n",
    "\n",
    "    epochs = [200]\n",
    "    batch_size = [2000]\n",
    "    param_grid = dict(activation = activation,\n",
    "                      momentum = momentum,\n",
    "                      learn_rate = learn_rate,\n",
    "                      decay_rate = decay_rate,\n",
    "                      dropout_rate = dropout_rate,\n",
    "                      neurons = neurons,\n",
    "                      epochs = epochs,\n",
    "                      batch_size = batch_size)\n",
    "    \n",
    "    model = KerasClassifier(build_fn=create_model)\n",
    "    grid = GridSearchCV(estimator=model, cv=kfolds, param_grid=param_grid, n_jobs=1)\n",
    "    grid_result = grid.fit(X, Y) \n",
    "    # summarize results\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    # Find best params. Run in separate process to make sure memory is deallocated afterwards.\n",
    "#     p = multiprocessing.Process(target=grid_search_learning_params)\n",
    "#     p = multiprocessing.Process(target=grid_search_neurons)\n",
    "    p = multiprocessing.Process(target=grid_search_activation)\n",
    "\n",
    "    p.start()\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Learning Parameters"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Best: 0.876000 using {'activation': 'relu', 'batch_size': 2000, 'decay_rate': 5e-05, 'dropout_rate': 0.0, 'epochs': 200, 'learn_rate': 0.5, 'momentum': 0.25, 'neurons': 20}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Neuron Count"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "{'activation': 'relu', 'batch_size': 2000, 'decay_rate': 5e-05, 'dropout_rate': 0, 'epochs': 200, 'learn_rate': 0.5, 'momentum': 0.25, 'neurons': 25}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Activation Function"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "{'activation': 'sigmoid', 'batch_size': 2000, 'decay_rate': 5e-05, 'dropout_rate': 0, 'epochs': 200, 'learn_rate': 0.5, 'momentum': 0.25, 'neurons': 25}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup ROC and AUC code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# ROC plot and AUC statistic, based on the code from the following source:\n",
    "########################################\n",
    "# Title: Tutorial 3-Titanic3 Cross-validation with ROC analysis\n",
    "# Author: Chuan Lu\n",
    "# Date: 06/04/2017\n",
    "# Code Version: 47c58c0\n",
    "# Available: https://github.com/aberML/CSM6420/\n",
    "########################################\n",
    "\n",
    "# (Hacked together, but does the job ;-) )\n",
    "mean_acc = 0.0\n",
    "mean_auc = 0.0\n",
    "all_tpr = []\n",
    "all_acc = []\n",
    "all_auc = []\n",
    "\n",
    "i=0\n",
    "\n",
    "def ROC_AUC(y_test, y_prob):\n",
    "    global i, mean_acc, mean_auc, all_tpr, all_acc, all_auc\n",
    "    i += 1\n",
    "    # Get prediction on class label from the model\n",
    "    y_prediction = np.around(y_prob, decimals=0)\n",
    "    \n",
    "    # Get probability output from the model\n",
    "    acc = np.sum(y_test == y_prediction)*1./len(y_test)\n",
    "    print(\"Prediction accuracy:\", acc)\n",
    "    # Compute ROC curve and area the curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print(\"Area under ROC curve (AUC):\", roc_auc)\n",
    "    plt.plot(fpr, tpr, lw=1, label='ROC fold %d (area = %0.2f)' % (i, roc_auc))    \n",
    "    all_acc.append(acc)\n",
    "    all_auc.append(roc_auc)\n",
    "    return acc, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_plot(title = 'Receiver operating characteristic example'):\n",
    "    global i, mean_acc, mean_auc, all_tpr, all_acc, all_auc\n",
    "    plt.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6))\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    all_acc=np.asarray(all_acc)\n",
    "    all_auc=np.asarray(all_auc)\n",
    "    print(all_acc)\n",
    "    # print 95% C.I. for both accuracy and AUC based on CV\n",
    "    print(\"Mean Accuracy: %0.3f (+/- %0.3f)\" % (all_acc.mean(), all_acc.std() * 1.96))\n",
    "    print(all_auc)\n",
    "    print(\"Mean AUC: %0.3f (+/- %0.3f)\" % (all_auc.mean(), all_auc.std() * 1.96))\n",
    "    \n",
    "    # Reset values for re-use.\n",
    "    mean_acc = 0.0\n",
    "    mean_auc = 0.0\n",
    "    all_tpr = []\n",
    "    all_acc = []\n",
    "    all_auc = []\n",
    "\n",
    "    i=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "from sklearn.cross_validation import StratifiedKFold as SKFold\n",
    "\n",
    "random_seed = 1234\n",
    "scv = SKFold(y=Y[:,0], n_folds=kfolds, random_state=random_seed)\n",
    "\n",
    "def optimal_model(samples):\n",
    "    global best_auc, best_acc\n",
    "    X_train, y_train, X_test, y_test = samples\n",
    "    # build the model\n",
    "    model = create_model(**{'activation': 'sigmoid',\n",
    "                            'decay_rate': 5e-05,\n",
    "                            'dropout_rate': 0,\n",
    "                            'learn_rate': 0.5,\n",
    "                            'momentum': 0.25,\n",
    "                            'neurons': 10})\n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=200, batch_size=2000, verbose=2)\n",
    "#     ROC_AUC(y_test[:,1], model.predict(X_test)[:,1])\n",
    "    \n",
    "    scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))\n",
    "    \n",
    "        \n",
    "    # Prepare test predictions for submission.\n",
    "    test = pd.read_csv(\"test.csv\")\n",
    "    X_sub, Id = test.iloc[:,1:], test[\"TestId\"]\n",
    "\n",
    "    Y_pred = model.predict_proba(X_sub)[:,1]\n",
    "\n",
    "    pred_df = pd.DataFrame(Id)\n",
    "    pred_df = pred_df.join(pd.DataFrame({\"PredictedScore\": Y_pred}))\n",
    "    \n",
    "    return y_test[:,1], model.predict_proba(X_test)[:,1], pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ROC_analysis():\n",
    "    best_acc = best_auc = 0\n",
    "    \n",
    "    dat_split = []\n",
    "    \n",
    "    for training_set, test_set in scv:  \n",
    "        X_train = X.iloc[training_set]\n",
    "        y_train = Y[training_set]\n",
    "        X_test = X.iloc[test_set]\n",
    "        y_test = Y[test_set]\n",
    "        \n",
    "        dat_split.append((X_train, y_train, X_test, y_test))\n",
    "        \n",
    "    \n",
    "    pool = multiprocessing.Pool(processes=1)\n",
    "    results = pool.map(optimal_model, dat_split)\n",
    "    \n",
    "    best_auc = best_acc = 0\n",
    "    \n",
    "    for y_test, y_pred, pred_df in results:\n",
    "        print(pred_df.shape)\n",
    "        acc, roc_auc = ROC_AUC(y_test, y_pred)\n",
    "\n",
    "        if acc + roc_auc > best_auc + best_acc:\n",
    "            # Save predictions for the best result.\n",
    "            best_acc = acc\n",
    "            best_auc = roc_auc\n",
    "            print(\"Using the fit with an acc of\", acc, \"and an AUC of\", roc_auc)\n",
    "            pred_df.to_csv(\"prednn.csv\", index = False)\n",
    "            \n",
    "    display_plot(\"Neural Network Receiver Operating Characteristic\")\n",
    "    plt.savefig(\"ROC_NN.pdf\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    ROC_analysis()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Mean Accuracy: 0.878 (+/- 0.034)\n",
    "Mean AUC: 0.901 (+/- 0.045)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/anaconda3/envs/anacenv/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python:  sys.version_info(major=3, minor=6, micro=5, releaselevel='final', serial=0)\n",
      "Pandas:  0.22.0\n",
      "Sklearn:  0.19.1\n",
      "Keras:  2.1.6\n",
      "tensorflow:  1.8.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import sklearn\n",
    "import keras\n",
    "import tensorflow\n",
    "import pandas as pd\n",
    "\n",
    "print('Python: ', sys.version_info)\n",
    "print('Pandas: ', pd.__version__)\n",
    "print('Sklearn: ', sklearn.__version__)\n",
    "print('Keras: ', keras.__version__)\n",
    "print('tensorflow: ', tensorflow.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anacenv]",
   "language": "python",
   "name": "conda-env-anacenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "nbpresent": {
   "slides": {},
   "themes": {
    "default": "6c86454f-30dc-4fba-9754-98e42f199839",
    "theme": {}
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
