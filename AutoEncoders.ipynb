{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensional reduction/ increase of data using autoencoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "d0eaf7ba-de3d-4918-a237-06c9d74b0f67"
    }
   },
   "outputs": [],
   "source": [
    "# Loading Data.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train = pd.read_csv(\"train.csv\", index_col=None)\n",
    "X, Y = train.iloc[:,1:], train[\"Class\"]\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the shallow neural network autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "397741be-9710-402c-be1f-7b1b93f05c7b"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop, SGD, Adamax\n",
    "\n",
    "# Simple autoencoder, based off the code from the following source:\n",
    "########################################\n",
    "# Title: Building Autoencoders in Keras\n",
    "# Author: Francois Chollet\n",
    "# Date: 14/05/2016\n",
    "# Available: https://blog.keras.io/building-autoencoders-in-keras.html\n",
    "########################################\n",
    "\n",
    "def get_encoder(encoding_dim = 32):\n",
    "    # this is our input placeholder\n",
    "    input_dat = Input(shape=(1024,))\n",
    "    # \"encoded\" is the encoded representation of the input\n",
    "    encoded = Dense(encoding_dim, activation='relu')(input_dat)\n",
    "    # \"decoded\" is the lossy reconstruction of the input\n",
    "    decoded = Dense(1024, activation='sigmoid')(encoded)\n",
    "\n",
    "    # this model maps an input to its reconstruction\n",
    "    autoencoder = Model(input_dat, decoded)\n",
    "    \n",
    "    # this model maps an input to its encoded representation\n",
    "    encoder = Model(input_dat, encoded)\n",
    "    \n",
    "    # create a placeholder for an encoded (32-dimensional) input\n",
    "    encoded_input = Input(shape=(encoding_dim,))\n",
    "    # retrieve the last layer of the autoencoder model\n",
    "    decoder_layer = autoencoder.layers[-1]\n",
    "    # create the decoder model\n",
    "    decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
    "    \n",
    "    optimiser = SGD(lr=0.5, decay=0.0)\n",
    "    autoencoder.compile(optimizer=optimiser, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "\n",
    "    autoencoder.fit(X_train, X_train,\n",
    "                    epochs=500,\n",
    "                    batch_size=200,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(X_test, X_test))\n",
    "    \n",
    "    return encoder\n",
    "    \n",
    "\n",
    "# Functions which can be spawned into new thread.\n",
    "def reduce_dims():\n",
    "    global X, Y\n",
    "    \n",
    "    encoder = get_encoder(512)\n",
    "    X_red = encoder.predict(X)\n",
    "    print(X_red.shape)\n",
    "    reduced = pd.DataFrame(Y)\n",
    "    reduced = reduced.join(pd.DataFrame(X_red))\n",
    "    reduced.to_csv(\"reduced_dims.csv\", index = False)\n",
    "    \n",
    "def increase_dims():\n",
    "    global X, Y\n",
    "    \n",
    "    encoder = get_encoder(2048)\n",
    "    X_inc = encoder.predict(X)\n",
    "\n",
    "    reduced = pd.DataFrame(Y)\n",
    "    reduced = reduced.join(pd.DataFrame(X_inc))\n",
    "    reduced.to_csv(\"increased_dims.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "28186300-42f7-4423-a690-0536dec14c70"
    }
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Spawn new threads, so that data is garbage collected.\n",
    "    p = multiprocessing.Process(target=reduce_dims)\n",
    "    p.start()\n",
    "    p.join()\n",
    "    \n",
    "    p = multiprocessing.Process(target=increase_dims)\n",
    "    p.start()\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the effectiveness of the encoded output using Random Forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a Random Forest classifier pipeline.\n",
    "#\n",
    "# Based on the code from the following source:\n",
    "########################################\n",
    "# Title: Model Comparison using Pipelines\n",
    "# Author: Chuan Lu\n",
    "# Date: 06/04/2017\n",
    "# Code Version: 47c58c0\n",
    "# Available: https://github.com/aberML/CSM6420/blob/master/Tutorial%203-Titanic5%20Building%20Pipelines%20and%20Model%20Comparison.ipynb\n",
    "#######################################\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# Loading Data.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "kfolds = 8\n",
    "\n",
    "anova_filter = SelectKBest(f_regression)\n",
    "# Random Forest Classifier.\n",
    "clf = RandomForestClassifier()\n",
    "pipeline_rf = Pipeline([\n",
    "    ('anova', anova_filter),\n",
    "    ('rf', clf)\n",
    "])\n",
    "\n",
    "def load_file(file):\n",
    "    train = pd.read_csv(file, index_col=None)\n",
    "    return train.iloc[:,1:], train[\"Class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_encoded(file = \"reduced_dims.csv\"):\n",
    "    global kfolds, pipeline_rf\n",
    "    \n",
    "    X, Y = load_file(file)\n",
    "    parameter_grid_rf = {\n",
    "                'anova__k': [10, 100, 'all'],\n",
    "                'rf__n_estimators': [10, 100, 1000],\n",
    "                'rf__max_depth': [5, 10, 50, 100, None],\n",
    "            }    \n",
    "    grid_search = GridSearchCV(pipeline_rf, parameter_grid_rf, cv=kfolds, verbose=3, n_jobs=8)\n",
    "    grid_search.fit(X, Y)\n",
    "\n",
    "    sorted(grid_search.grid_scores_, key=lambda x: x.mean_validation_score)\n",
    "    print(grid_search.best_score_)\n",
    "    print(grid_search.best_params_)\n",
    "    \n",
    "test_encoded()\n",
    "test_encoded(\"increased_dims.csv\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Best predictor and selected parameters for dimensional reduction:\n",
    "0.857\n",
    "{'anova__k': 100, 'rf__max_depth': 50, 'rf__n_estimators': 100}\n",
    "\n",
    "Best predictor and selected parameters for dimensional increase:\n",
    "0.861\n",
    "{'anova__k': 'all', 'rf__max_depth': 100, 'rf__n_estimators': 100}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot ROC curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# ROC plot and AUC statistic, based on the code from the following source:\n",
    "########################################\n",
    "# Title: Cross-Validation and ROC curve analysis\n",
    "# Author: Chuan Lu\n",
    "# Date: 06/04/2017\n",
    "# Available: https://github.com/aberML/CSM6420/\n",
    "########################################\n",
    "# (Hacked together, but does the job ;-) )\n",
    "mean_acc = 0.0\n",
    "mean_auc = 0.0\n",
    "all_tpr = []\n",
    "all_acc = []\n",
    "all_auc = []\n",
    "\n",
    "i=0\n",
    "\n",
    "def ROC_AUC(y_test, y_prob):\n",
    "    global i, mean_acc, mean_auc, all_tpr, all_acc, all_auc\n",
    "    i += 1\n",
    "    # Get prediction on class label from the model\n",
    "    y_prediction = np.around(y_prob, decimals=0)\n",
    "    \n",
    "    # Get probability output from the model\n",
    "    acc = np.sum(y_test == y_prediction)*1./len(y_test)\n",
    "    print(\"Prediction accuracy:\", acc)\n",
    "    # Compute ROC curve and area the curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print(\"Area under ROC curve (AUC):\", roc_auc)\n",
    "    plt.plot(fpr, tpr, lw=1, label='ROC fold %d (area = %0.2f)' % (i, roc_auc))    \n",
    "    all_acc.append(acc)\n",
    "    all_auc.append(roc_auc)\n",
    "\n",
    "def display_plot(title = 'Receiver operating characteristic example'):\n",
    "    global i, mean_acc, mean_auc, all_tpr, all_acc, all_auc\n",
    "    plt.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6))#, label='Luck')\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    all_acc=np.asarray(all_acc)\n",
    "    all_auc=np.asarray(all_auc)\n",
    "    print(all_acc)\n",
    "    # print 95% C.I. for both accuracy and AUC based on CV\n",
    "    print(\"Mean Accuracy: %0.3f (+/- %0.3f)\" % (all_acc.mean(), all_acc.std() * 1.96))\n",
    "    print(all_auc)\n",
    "    print(\"Mean AUC: %0.3f (+/- %0.3f)\" % (all_auc.mean(), all_auc.std() * 1.96))\n",
    "    \n",
    "    # Reset values for re-use.\n",
    "    mean_acc = 0.0\n",
    "    mean_auc = 0.0\n",
    "    all_tpr = []\n",
    "    all_acc = []\n",
    "    all_auc = []\n",
    "\n",
    "    i=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check ROC results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold as SKFold\n",
    "\n",
    "random_seed = 1234\n",
    "scv = SKFold(y=Y, n_folds=kfolds, random_state=random_seed)\n",
    "\n",
    "input_len, input_sz = X.shape[0], X.shape[1]\n",
    "\n",
    "# train_frac = 0.5\n",
    "# sep = int(input_len*train_frac)\n",
    "for enc, title, params in [(\"reduced_dims\",\n",
    "                            \"Random Forest Dimension Reduction ROC\",\n",
    "                            {'anova__k': 'all', 'rf__max_depth': 50, 'rf__n_estimators': 100}),\n",
    "                           (\"increased_dims\",\n",
    "                            \"Random Forest Dimension Increase ROC\", \n",
    "                            {'anova__k': 100, 'rf__max_depth': 100, 'rf__n_estimators': 100})]:\n",
    "    \n",
    "    X, Y = load_file(enc+\".csv\")\n",
    "    plt.figure()\n",
    "    for training_set, test_set in scv:  \n",
    "        X_train = X.iloc[training_set]\n",
    "        y_train = Y.iloc[training_set]\n",
    "        X_test = X.iloc[test_set]\n",
    "        y_test = Y.iloc[test_set]\n",
    "        print(\"Shape of training:\")\n",
    "        print(\"X:\", X_train.shape, \"y:\", y_train.shape)\n",
    "        print(\"Shape of testing:\")\n",
    "        print(\"X:\", X_test.shape, \"y:\", y_test.shape)\n",
    "\n",
    "        # pipeline_rf.set_params(**{'anova__k': 'all', 'rf__max_depth': 50, 'rf__n_estimators': 1000})\n",
    "        # pipeline_rf.fit(X_train, y_train)\n",
    "\n",
    "        pipeline_rf.set_params(**params)\n",
    "        pipeline_rf.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = pipeline_rf.predict_proba(X_test)[:,1]\n",
    "\n",
    "\n",
    "        ROC_AUC(y_test, y_pred)\n",
    "\n",
    "\n",
    "    display_plot(title)\n",
    "    plt.savefig(\"rf_\" + enc + \".pdf\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Reduced dimensions results:\n",
    "Mean Accuracy: 0.856 (+/- 0.059)\n",
    "Mean AUC: 0.862 (+/- 0.126) \n",
    "Increased dimensions results:\n",
    "Mean Accuracy: 0.855 (+/- 0.055)\n",
    "Mean AUC: 0.857 (+/- 0.106)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python:  sys.version_info(major=3, minor=6, micro=5, releaselevel='final', serial=0)\n",
      "Pandas:  0.22.0\n",
      "Sklearn:  0.19.1\n",
      "Keras:  2.1.6\n",
      "tensorflow:  1.8.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import sklearn\n",
    "import keras\n",
    "import tensorflow\n",
    "import pandas as pd\n",
    "\n",
    "print('Python: ', sys.version_info)\n",
    "print('Pandas: ', pd.__version__)\n",
    "print('Sklearn: ', sklearn.__version__)\n",
    "print('Keras: ', keras.__version__)\n",
    "print('tensorflow: ', tensorflow.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anacenv]",
   "language": "python",
   "name": "conda-env-anacenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
